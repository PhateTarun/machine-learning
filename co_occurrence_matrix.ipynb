{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "co-occurrence_matrix.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCeB_aEd5uQ1",
        "colab_type": "text"
      },
      "source": [
        "Co-Occurrence matrix on TFIDF text vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ893buU5uQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter \n",
        "from math import log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SDCCWmKHAPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(feature_names, tf_idf, top_features):\n",
        "    features_dict = dict.fromkeys(Counter(feature_names),0)\n",
        "    for key, index in zip(feature_names, range(len(feature_names))):\n",
        "        features_dict[key] = max(tf_idf[index])\n",
        "    if top_features is not True:\n",
        "        feature_names = []\n",
        "        for i in range(top_features):\n",
        "            temp = max(features_dict, key=features_dict.get)\n",
        "            del features_dict[temp]\n",
        "            feature_names.append(temp)\n",
        "    return  feature_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzy5aIBP5uRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf_vector(doc_list, top_features = False):\n",
        "    feature_names = list(set(' '.join(doc_list).split()))  # getting all the unique words in all documents\n",
        "    no_of_docs = len(doc_list)\n",
        "    # creating the numpy array where \n",
        "    # rows=No. of unique words in all document\n",
        "    # columns = No. of documents\n",
        "    tf_idf = np.full(((len(feature_names)), (no_of_docs)),0, dtype=object)  # creating the numpy array where rows=No. of unique words in all document\n",
        "    \n",
        "    word_count = [len(i.split()) for i in doc_list]        # counting number of words in each document\n",
        "    word_counter = [Counter(i.split()) for i in doc_list]  # getting no. of times a word appeared in each document\n",
        "    # calculating tfidf value for each word corresponding to each document \n",
        "    for name, word_index in zip(feature_names, range(len(feature_names))):         \n",
        "        term_freq = 0\n",
        "        for document in doc_list:                           # getting, in how may documents a word is present\n",
        "            if name in document.split():\n",
        "                term_freq+=1\n",
        "        idf_value = log(no_of_docs)/(log(term_freq)+1)      # calculating idf value for the word\n",
        "        for doc_no in range(len(doc_list)):\n",
        "            tf_idf[word_index][doc_no] = ((word_counter[doc_no][name])/word_count[doc_no])*idf_value  \n",
        "    if top_features is not False:\n",
        "        return tf_idf, get_features(feature_names, tf_idf, top_features)\n",
        "\n",
        "    return tf_idf\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1hUpJbe5uRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "9b52ae8c-e5e1-4b36-e28b-68035e1c76ae"
      },
      "source": [
        "a = [\"abc def ijk pqr\", \"pqr klm opq\", \"lmn pqr xyz abc def pqr abc\", 'go goa gon', 'aaja, fdads']\n",
        "import pandas as pd\n",
        "tfidf, feature_names = tfidf_vector(a, top_features=3) \n",
        "pd.DataFrame(tfidf_vector(a))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.536479</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.402359</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.536479</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.23764</td>\n",
              "      <td>0</td>\n",
              "      <td>0.271589</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.536479</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.22992</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.22992</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.191726</td>\n",
              "      <td>0.255635</td>\n",
              "      <td>0.219116</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.536479</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.804719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.536479</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.804719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.23764</td>\n",
              "      <td>0</td>\n",
              "      <td>0.135794</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2         3         4\n",
              "0          0  0.536479         0         0         0\n",
              "1   0.402359         0         0         0         0\n",
              "2          0         0         0  0.536479         0\n",
              "3    0.23764         0  0.271589         0         0\n",
              "4          0  0.536479         0         0         0\n",
              "5          0         0   0.22992         0         0\n",
              "6          0         0   0.22992         0         0\n",
              "7   0.191726  0.255635  0.219116         0         0\n",
              "8          0         0         0  0.536479         0\n",
              "9          0         0         0         0  0.804719\n",
              "10         0         0         0  0.536479         0\n",
              "11         0         0         0         0  0.804719\n",
              "12   0.23764         0  0.135794         0         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hISanMj5uRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "053bad80-1295-4e74-fc81-181c62b9e909"
      },
      "source": [
        "word=[]\n",
        "for i in tqdm(a):\n",
        "    temp = []\n",
        "    for s in i.split():\n",
        "        temp.append(s)\n",
        "    word.append(temp)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 8651.62it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9vGMy-t5uR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getallindex(list, string):\n",
        "    '''returns all the index if word is in the list'''\n",
        "    return filter(lambda a: list[a]==string, range(0,len(list)))\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBzypSL_5uSG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "2eafef10-d93f-48c7-fd17-7513265b7787"
      },
      "source": [
        "co_mat = np.full((4,4),-1, dtype=object)\n",
        "for i in range(0,4):\n",
        "    if i==0:\n",
        "        co_mat[i,i]=0\n",
        "        continue\n",
        "    co_mat[0,i]=feature_names[(i-1)]\n",
        "    co_mat[i,0]=feature_names[(i-1)]\n",
        "co_mat"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 'aaja,', 'fdads', 'opq'],\n",
              "       ['aaja,', -1, -1, -1],\n",
              "       ['fdads', -1, -1, -1],\n",
              "       ['opq', -1, -1, -1]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V270dsLv5uSL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ad148b2-6ecf-46f7-ac3a-408c37b57ba8"
      },
      "source": [
        "l=1\n",
        "\n",
        "for feature in tqdm(feature_names):\n",
        "    temp=[]\n",
        "    # on every occurrence of word getting its previous 4 words and next 4 words\n",
        "    for essay in word:                   \n",
        "        for i in getallindex(essay,feature):\n",
        "            backward=''\n",
        "            forward = ''\n",
        "            for a in range(0,2):\n",
        "                if (i-a)>=0:\n",
        "                    backward+=essay[i-a]+' '\n",
        "                if (i+a)<=(len(essay)-1):\n",
        "                    forward+=essay[i+a]+' '\n",
        "            if backward!='':\n",
        "                temp.append(backward)\n",
        "            if forward!='':\n",
        "                temp.append(forward)\n",
        "    c=1\n",
        "    # calulating co-occerrence of word for all the windows\n",
        "    for x in feature_names:\n",
        "        count=0\n",
        "        if (l==c):\n",
        "            co_mat[l,c]=0\n",
        "        elif (co_mat[l,c]==-1):\n",
        "            for t in temp:\n",
        "                if (x in t):\n",
        "                    count+=1\n",
        "            co_mat[l,c]=count\n",
        "            co_mat[c,l]=count\n",
        "        c+=1\n",
        "    l+=1\n",
        "        "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 10932.16it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsWVIdzr5uSW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "0a17c106-214b-4c89-ccf1-d5c7e4989bc3"
      },
      "source": [
        "co_mat"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 'aaja,', 'fdads', 'opq'],\n",
              "       ['aaja,', 0, 1, 0],\n",
              "       ['fdads', 1, 0, 0],\n",
              "       ['opq', 0, 0, 0]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6784FrJ_5uSa",
        "colab_type": "text"
      },
      "source": [
        "The above is on context window size 2 and all diagonal element are zero.\n",
        "i referred to the below  link\n",
        "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAVpxKsq5uSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}